{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc15d61e",
   "metadata": {},
   "source": [
    "Similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988fc548",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#import texts and embedding df\n",
    "text_chunks_and_embeddings_df = pd.read_csv(\"text_chunks_and_embeddings_df.cs\n",
    "v\")\n",
    "#convert embedding column back to np.array\n",
    "text_chunks_and_embeddings_df[\"embedding\"] = text_chunks_and_embeddings_df[\"e\n",
    "mbedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
    "#convert the embeddings to torch.tensor\n",
    "embeddings = torch.tensor(np.stack(text_chunks_and_embeddings_df[\"embedding\"]\n",
    ".tolist(), axis=0), dtype=torch.float32).to(device)\n",
    "#convert texts and embedding df to list of dicts\n",
    "files_and_chunks = text_chunks_and_embeddings_df.to_dict(orient=\"records\")\n",
    "text_chunks_and_embeddings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a69d5c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d86a46",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#create model\n",
    "from sentence_transformers import util, SentenceTransformer\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\",\n",
    "device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05b8d55",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#1. define query\n",
    "query = \"merge request\"\n",
    "print(f\"Query: {query}\")\n",
    "#2. embed the query WITH SAME MODEL AS TEXT EMBEDDED WITH\n",
    "query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "#get dot product similarity score\n",
    "from time import perf_counter as timer\n",
    "start_time = timer()\n",
    "dot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]\n",
    "end_time = timer()\n",
    "print(f\"[INFO] Time taken to get scores on {len(embeddings)} embeddings: {end\n",
    "_time-start_time:.5f} seconds.\")\n",
    "#4. get top-k results (top 5)\n",
    "top_results_dot_product = torch.topk(dot_scores, k=5)\n",
    "top_results_dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1ead50",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "files_and_chunks[98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d9a949",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#displaying the vector search result nicer\n",
    "import textwrap\n",
    "def print_wrapped(text, wrap_lenght=80):\n",
    "wrapped_text = textwrap.fill(text, wrap_lenght)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdc0028",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "query = \"merge request\"\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Results:\")\n",
    "#loop through zipped together scores and indices from torch.topk\n",
    "for score, idx in zip(top_results_dot_product[0], top_results_dot_product[1])\n",
    ":\n",
    "print(f\"Score: {score:.4f}\")\n",
    "print(\"Text:\")\n",
    "print_wrapped(files_and_chunks[idx][\"sentence_chunk\"])\n",
    "print(f\"File name: {files_and_chunks[idx]['filename']}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407759c5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "file_path = 'content_pull_request/approving-a-pull-request-with-required-revi\n",
    "ews.md'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "markdown_text = file.read()\n",
    "display(Markdown(markdown_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eee958",
   "metadata": {},
   "source": [
    "Vector similarity\n",
    "\n",
    "Functionizing the semantic search pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab0b086",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def retrieve_relevant_resources(query: str,\n",
    "embeddings: torch.tensor,\n",
    "model: SentenceTransformer=embedding_model,\n",
    "n_resources_to_return: int=5,\n",
    "print_time: bool=True):\n",
    "# embedding the query with model and returning top k scores and indices f\n",
    "rom the embedding:\n",
    "query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "#dot product\n",
    "start_time = timer()\n",
    "dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
    "end_time = timer()\n",
    "if print_time:\n",
    "print(f\"[INFO] Time taken to get scores on ({len(embeddings)}) embedd\n",
    "ings: {end_time-start_time:.5f} seconds.\")\n",
    "scores, indices = torch.topk(input=dot_scores,\n",
    "k=n_resources_to_return)\n",
    "return scores, indices\n",
    "def print_top_results_and_scores(query: str,\n",
    "embeddings: torch.tensor,\n",
    "files_and_chunks: list[dict]=files_and_chunks\n",
    ",\n",
    "n_resources_to_return: int=5):\n",
    "#finds relevant paragraphs for query and prints them with their scores\n",
    "scores, indices = retrieve_relevant_resources(query=query,\n",
    "embeddings=embeddings,\n",
    "n_resources_to_return=n_resou\n",
    "rces_to_return)\n",
    "#loop through zipped together scores and indices from torch.topk\n",
    "for score, idx in zip(scores, indices):\n",
    "print(f\"Score: {score:.4f}\")\n",
    "print(\"Text:\")\n",
    "print_wrapped(files_and_chunks[idx][\"sentence_chunk\"])\n",
    "print(f\"File name: {files_and_chunks[idx]['filename']}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5890a36e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "query=\"merge comments\"\n",
    "#retrieve_relevant_resources(query=query, embeddings=embeddings)\n",
    "print_top_results_and_scores(query=query, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03cc38e",
   "metadata": {},
   "source": [
    "Local generative LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0ef84c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6afb05",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory\n",
    "gpu_memory_gb = round(gpu_memory_bytes / (2**30))\n",
    "print(f\"Available GPU memory: {gpu_memory_gb} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ae7119",
   "metadata": {},
   "source": [
    "Load the LLM locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8b5eb4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "#quantization config\n",
    "from transformers import BitsAndBytesConfig\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "bnb_4bit_compute_dtype=torch.\n",
    "float16)\n",
    "attn_implementation = \"sdpa\" #scaled dot product attention\n",
    "#select model to use (using local as not able to login to huggingface CLI\n",
    "model_directory = \"C:/Users/MÃ¡rk/google_gemma\"\n",
    "#tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
    "#instantiate LLM\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(model_directory,\n",
    "torch_dtype=torch.float16,\n",
    "quantization_config=quantiza\n",
    "tion_config if quantization_config else None,\n",
    "low_cpu_mem_usage=False,\n",
    "attn_implementation=attn_imp\n",
    "lementation)\n",
    "if not quantization_config:\n",
    "llm_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac582ce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "llm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e511bc9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#get the number of parameters of the model gemma-2b-it\n",
    "def get_model_num_params(model: torch.nn.Module):\n",
    "return sum([param.numel() for param in model.parameters()])\n",
    "get_model_num_params(llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3559f99c",
   "metadata": {},
   "source": [
    "Generate text with the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5172a07f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "input_text = \"How to comment on a pull request?\"\n",
    "print(f\"Input text:\\n{input_text}\")\n",
    "#create prompt template\n",
    "dialogue_template = [\n",
    "{\"role\": \"user\",\n",
    "\"content\": input_text}\n",
    "]\n",
    "#apply template\n",
    "prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "tokenize=False,\n",
    "add_generation_prompt=True)\n",
    "print(f\"\\nPrompt (formatted): \\n{prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62440fe9",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea464185",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718dcea0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#tokenize prompt and send it to the GPU\n",
    "input_ids = tokenizer(prompt,\n",
    "return_tensors=\"pt\").to(\"cuda\")\n",
    "#generate output for the LLM\n",
    "outputs = llm_model.generate(**input_ids,\n",
    "max_new_tokens=256)\n",
    "print(f\"Model output (tokens):\\n{outputs[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac7c1bf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#convert the output tokens back to text\n",
    "outputs_decoded = tokenizer.decode(outputs[0])\n",
    "print(f\"Model output (decoded):\\n{outputs_decoded}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8054fa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#question list to test the model\n",
    "question_list = [\"What is a pull request in GitHub, and how does it contribut\n",
    "e to the collaborative development process?\",\n",
    "\"Can you explain the steps to resolve merge conflicts in a G\n",
    "itHub pull request?\",\n",
    "\"What are the best practices for reviewing a pull request on\n",
    "GitHub?\",\n",
    "\"How can you link a pull request to an issue in GitHub to au\n",
    "tomate issue closure upon merging?\",\n",
    "\"What are some common mistakes to avoid when creating a pull\n",
    "request on GitHub?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7937507",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "query = random.choice(question_list)\n",
    "print(f\"Query: {query}\")\n",
    "#see the scores of top related results\n",
    "scores, indices = retrieve_relevant_resources(query=query,\n",
    "embeddings=embeddings)\n",
    "scores, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3102776c",
   "metadata": {},
   "source": [
    "Augmenting our prompt with context items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6952cb39",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def prompt_formatter(query: str,\n",
    "context_items: list[dict]) -> str:\n",
    "context =\n",
    "\"\n",
    "-\n",
    "\" + \"\\n-\n",
    "\".join([item[\"sentence_chunk\"] for item in context_\n",
    "items])\n",
    "base_prompt = f\"\"\"Based on the following context items, please answer the\n",
    "query.\n",
    "The answers should be explanatory and comprehensive.\n",
    "Query: {query}\n",
    "Context items:\n",
    "{context}\n",
    "Relevant parts: <extract relevant passages from the context>\n",
    "Answer:\n",
    "\"\"\"\n",
    "base_prompt = base_prompt.format(context=context,\n",
    "query=query)\n",
    "#prompt template for it model\n",
    "dialogue_template = [\n",
    "{\"role\": \"user\",\n",
    "\"content\": base_prompt}\n",
    "]\n",
    "#apply template\n",
    "prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "tokenize=False,\n",
    "add_generation_prompt=True)\n",
    "return base_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e60bdd9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "query = random.choice(question_list)\n",
    "print(f\"Query: {query}\")\n",
    "scores, induces = retrieve_relevant_resources(query=query,\n",
    "embeddings=embeddings)\n",
    "context_items = [files_and_chunks[i] for i in indices]\n",
    "prompt = prompt_formatter(query=query,\n",
    "context_items=context_items)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d3b485",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = llm_model.generate(**input_ids,\n",
    "temperature=0.7, #higher the temperature, the mo\n",
    "re creative the answer is\n",
    "do_sample=True,\n",
    "max_new_tokens=256)\n",
    "output_text = tokenizer.decode(outputs[0])\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"RAG answer:\\n{output_text.replace(prompt, '')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6b874a",
   "metadata": {},
   "source": [
    "Create a function about the answer of the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a29652",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def ask(query: str,\n",
    "temperature: float=0.5,\n",
    "max_new_tokens: int=256,\n",
    "format_answer_text=True,\n",
    "detailed_output=True):\n",
    "#RETRIEVAL\n",
    "#get scores and indices of top results\n",
    "scores, indices = retrieve_relevant_resources(query=query,\n",
    "embeddings=embeddings)\n",
    "#create context items list\n",
    "context_items = [files_and_chunks[i] for i in indices]\n",
    "#add score to item\n",
    "for i, item in enumerate(context_items):\n",
    "item[\"score\"] = scores[i].cpu()\n",
    "if not detailed_output:\n",
    "item.pop(\"embedding\", None) #remove embeddings if detailed_output\n",
    "=False\n",
    "#AUGMENTATION\n",
    "#create the propmt and format it\n",
    "prompt = prompt_formatter(query=query,\n",
    "context_items=context_items)\n",
    "#GENERATION\n",
    "#tokenize prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "#generate output tokens\n",
    "outputs = llm_model.generate(**input_ids,\n",
    "temperature=temperature,\n",
    "do_sample=True,\n",
    "max_new_tokens=max_new_tokens)\n",
    "#decode tokens back to text\n",
    "output_text = tokenizer.decode(outputs[0]).replace(\"<bos>\",\n",
    "'').replace(\"\n",
    "<eos>\",\n",
    "'')\n",
    "#prettyfy answer\n",
    "if format_answer_text:\n",
    "print(f\"RAG answer:\\n{output_text.replace(prompt, '')}\")\n",
    "if detailed_output:\n",
    "return context_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a6b863",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "query = random.choice(question_list)\n",
    "print(f\"Query: {query}\")\n",
    "ask(query=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c18ff14",
   "metadata": {},
   "source": [
    "Testing:\n",
    "\n",
    "Short form in the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf92a1b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "give_query = \"How to merge a PR?\"\n",
    "if give_query == \" \":\n",
    "query = random.choice(question_list)\n",
    "else:\n",
    "query = give_query\n",
    "print(f\"Query: {query}\")\n",
    "ask(query=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213b1a58",
   "metadata": {},
   "source": [
    "Irrelevant question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaed4bb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "give_query = \"How long does a koala live?\"\n",
    "if give_query == \" \":\n",
    "query = random.choice(question_list)\n",
    "else:\n",
    "query = give_query\n",
    "print(f\"Query: {query}\")\n",
    "ask(query=query)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
