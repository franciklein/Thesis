{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4acff86",
   "metadata": {},
   "source": [
    "Providing and cleaning up the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ae4d6a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "def text_formatter_md(text: str) -> str:\n",
    "\"\"\"formatting md\"\"\"\n",
    "cleaned_text = text.replace(\"\\n\", \" \").strip()\n",
    "return cleaned_text\n",
    "def open_and_read_md_folder(folder_path: str) -> list[dict]:\n",
    "all_texts = []\n",
    "for filename in os.listdir(folder_path):\n",
    "if filename.endswith(\".md\"):\n",
    "md_path = os.path.join(folder_path, filename)\n",
    "with open(md_path, 'r', encoding='utf-8') as file:\n",
    "text = file.read()\n",
    "text = text_formatter_md(text=text)\n",
    "all_texts.append({\n",
    "\"filename\": filename,\n",
    "\"char_count\": len(text),\n",
    "\"word_count\": len(text.split(\" \")),\n",
    "\"sentence_count_raw\": len(text.split(\". \")),\n",
    "\"token_count\": len(text) / 5, # subword tokenizing average es\n",
    "timate in English is ~ 5\n",
    "\"text\": text\n",
    "})\n",
    "return all_texts\n",
    "# Specify the path to your folder containing .md files\n",
    "folder_path = \"content_pull_request\"\n",
    "texts = open_and_read_md_folder(folder_path=folder_path)\n",
    "#check some examples\n",
    "random.sample(texts, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c1e44a",
   "metadata": {},
   "source": [
    "Displaying the document details for the first 5 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cc5df8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(texts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6485fe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8810168c",
   "metadata": {},
   "source": [
    "Splitting the text by sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a4bbb7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install -qU langchain-text-splitters\n",
    "from langchain_text_splitters import Language, RecursiveCharacterTextSplitter\n",
    "# Initialize the Markdown splitter\n",
    "md_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "language=Language.MARKDOWN, chunk_size=100, chunk_overlap=0\n",
    ")\n",
    "for item in tqdm(texts):\n",
    "# Split the markdown content into chunks\n",
    "md_docs = md_splitter.create_documents([item[\"text\"]])\n",
    "# Initialize an empty list to store sentences after splitting\n",
    "item[\"sentences\"] = []\n",
    "# Extract text from each chunk to form sentences\n",
    "for doc in md_docs:\n",
    "doc_text = doc.page_content # Access page content directly\n",
    "#use the below option when not using stop-word removal\n",
    "item[\"sentences\"].append(doc_text)\n",
    "# Further process with spaCy to handle stopwords or refine text\n",
    "#nlp_doc = nlp(doc_text)\n",
    "#filtered_sentence = \" \".join(token.text for token in nlp_doc if not\n",
    "token.is_stop)\n",
    "#item[\"sentences\"].append(filtered_sentence)\n",
    "# Count the number of sentences\n",
    "item[\"sentence_count_md\"] = len(item[\"sentences\"])\n",
    "# Ensure all sentences are strings\n",
    "item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c581cd86",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "random.sample(texts, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01da2035",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(texts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e937d2a7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(texts)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015f8208",
   "metadata": {},
   "source": [
    "Splitting the text to sentence chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e81163",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define split size\n",
    "num_sentence_chunk_size = 18\n",
    "# Create function to split text recursively\n",
    "def split_list(input_list: list,\n",
    "slice_size: int=num_sentence_chunk_size) -> list[list[str]]:\n",
    "return [input_list[i:i + slice_size] for i in range(0, len(input_list), s\n",
    "lice_size)]\n",
    "test_list = list(range(25))\n",
    "split_list(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e42ff9b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Apply for the git documents and split sentences into chunks\n",
    "for item in tqdm(texts):\n",
    "item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"],\n",
    "slice_size=num_sentence_chunk_size)\n",
    "item[\"num_chunks\"] = len(item[\"sentence_chunks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed7aa9f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "random.sample(texts, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a285e7a2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(texts)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbe799b",
   "metadata": {},
   "source": [
    "Splitting each chunk to a new item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc78b68e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "#split each chunk into its own item\n",
    "files_and_chunks = []\n",
    "for item in tqdm(texts):\n",
    "for sentence_chunk in item[\"sentence_chunks\"]:\n",
    "chunk_dict = {}\n",
    "chunk_dict[\"filename\"] = item[\"filename\"]\n",
    "#join the sentences back together into paragraph-like structure\n",
    "joined_sentence_chunk =\n",
    "\"\".join(sentence_chunk).replace(\" \", \" \").st\n",
    "rip()\n",
    "joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence\n",
    "_chunk) # add a space to any full stop which is followed by a capital letter:\n",
    "\".A\" -> \". A\"\n",
    "#joined_sentence_chunk =\n",
    "chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
    "#statistics\n",
    "chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
    "chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentenc\n",
    "e_chunk.split(\" \")])\n",
    "chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 5 # le\n",
    "t's assume with subword tokenizing, 1 token = ~ 5 chars\n",
    "files_and_chunks.append(chunk_dict)\n",
    "len(files_and_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd5a75f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "random.sample(files_and_chunks, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3da949",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(files_and_chunks)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205330e0",
   "metadata": {},
   "source": [
    "Embedding the text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c35e37f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#vector size of embedding model\n",
    "embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d254181",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d622d09e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#prerequisite: pip install -U sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\")\n",
    "embedding_model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#embedd chunks one by one\n",
    "for item in tqdm(files_and_chunks):\n",
    "item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea8b547",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Turn text chunks into a single list\n",
    "text_chunks = [item[\"sentence_chunk\"] for item in files_and_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c7d0b2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Embed all texts in batches\n",
    "text_chunk_embeddings = embedding_model.encode(text_chunks,\n",
    "batch_size=32, #Select the bat\n",
    "ch size that works best\n",
    "convert_to_tensor=True) #to re\n",
    "turn embeddings as tensor instead of array\n",
    "text_chunk_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66029f9",
   "metadata": {},
   "source": [
    "Save the embeddings to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28af7374",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#display an example file:\n",
    "files_and_chunks[119]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c51331",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# create a single .csv file from the embedded chunk list\n",
    "text_chunks_and_embeddings_df = pd.DataFrame(files_and_chunks)\n",
    "embeddings_df_save_path = \"text_chunks_and_embeddings_df.csv\"\n",
    "text_chunks_and_embeddings_df.to_csv(embeddings_df_save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610ae772",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#import the saved file and view it\n",
    "text_chunks_and_embeddings_df_load = pd.read_csv(embeddings_df_save_path)\n",
    "text_chunks_and_embeddings_df_load.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba047cf6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
